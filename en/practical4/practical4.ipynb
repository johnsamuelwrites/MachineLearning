{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e542386b-0558-4e40-b52c-de17d2181f85",
   "metadata": {},
   "source": [
    "# Practical 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673e9851-e863-493d-9526-6f6aaf4aa8af",
   "metadata": {},
   "source": [
    "## Goals\n",
    "\n",
    "1.  Understand text analysis techniques, including stemming, lemmatization, and morphological analysis.\n",
    "2.  Practice extracting structured elements from HTML (links, images, tables, sections).\n",
    "3.  Apply NLP tools to explore a real-world text corpus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582e0b36-39be-4ec6-ade3-387b77f5f5b4",
   "metadata": {},
   "source": [
    "## Exercise 4.1 \\[★\\]\n",
    "\n",
    "Download this webpage of Wikipedia: https://fr.wikipedia.org/wiki/Paris and save the file as an HTML. Analyze the Wikipedia page by extracting and counting words, links, images, numbers, dates, proper nouns, and structured data from tables, while differentiating between sections and paragraphs. This involves downloading the HTML, parsing it, and systematically identifying relevant content. Write a program to implement these tasks: \n",
    "\n",
    "1. **Download HTML**: Fetch and save the Wikipedia page as an HTML file.\n",
    "2. **Load Content**: Read and parse the HTML file for analysis.\n",
    "3. **Word Analysis**: Count word occurrences in the text.\n",
    "4. **Extract Links**: Identify and categorize internal and external links.\n",
    "5. **Image Extraction**: Locate images and gather their URLs and sizes.\n",
    "6. **Number and Date Extraction**: Identify numbers, dates, and geographical coordinates.\n",
    "7. **Proper Nouns**: Extract names of people and places.\n",
    "8. **Table Data**: Locate and extract data from tables.\n",
    "9. **Section Differentiation**: Identify sections and paragraphs in the content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f46833e-4bce-4dc9-8add-89f44a5c7492",
   "metadata": {},
   "source": [
    "#### Analysis of Wikipedia Page: Paris\n",
    "\n",
    "In this notebook, tasks will be performed to extract and analyze various elements from the Wikipedia page of Paris.\n",
    "\n",
    "##### Step 1: Download the HTML Page\n",
    "First, download the HTML content of the specified Wikipedia page and save it as an HTML file. We use the `requests` library to handle the HTTP request. Remember to check the response status to confirm that the page was downloaded successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0ab6df-7131-4d27-87de-ea431191a320",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# URL of the Wikipedia page\n",
    "url = \"https://fr.wikipedia.org/wiki/Paris\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Save the content as an HTML file\n",
    "with open(\"paris.html\", \"w\", encoding='utf-8') as file:\n",
    "    file.write(response.text)\n",
    "\n",
    "print(\"HTML page downloaded and saved as paris.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ebbfa5-7cff-41ab-818d-b94338ca42c5",
   "metadata": {},
   "source": [
    "##### Step 2: Load the HTML Content\n",
    "Load the downloaded HTML file for further analysis.\n",
    "- **Comment**: Parsing the HTML is crucial for extracting data. Make sure to use a library like BeautifulSoup that can navigate the HTML structure effectively.\n",
    "\n",
    "Familiarize yourself with the `BeautifulSoup` methods to find elements in the HTML, such as `find()` and `find_all()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef264ae1-81a0-4975-ae4c-b689d8103dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load the HTML file\n",
    "with open(\"paris.html\", \"r\", encoding='utf-8') as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "print(\"HTML content loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fe0893-327f-4da6-bd18-d17288d044e6",
   "metadata": {},
   "source": [
    "##### Step 3: Extract and Analyze Words\n",
    "Count the occurrences of each word in the page.\n",
    "- **Comment**: Consider normalizing the text by converting it to lowercase to avoid counting the same word in different cases separately. We use regular expressions to effectively filter out non-word characters when splitting the text into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b552f2d2-11ab-4f38-80b5-dbd900f20e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Extract text from the HTML content\n",
    "text = soup.get_text()\n",
    "\n",
    "# Clean and split text into words\n",
    "words = re.findall(r'\\w+', text.lower())\n",
    "word_count = Counter(words)\n",
    "\n",
    "# Display the 10 most common words\n",
    "print(word_count.most_common(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6c8782-5dfb-4bb2-a9f2-a10e365daf7b",
   "metadata": {},
   "source": [
    "##### Step 4: Extract Links\n",
    "Identify all internal and external links from the page.\n",
    "\n",
    "- **Comment**: Understanding the difference between internal and external links is important for categorization.\n",
    "- **Hint**: Check the `href` attribute of the anchor (`<a>`) tags to determine the type of link."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7e9cae-5524-48cf-9801-25a3a5c89056",
   "metadata": {},
   "source": [
    "##### Step 5: Extract Images and Their Sizes\n",
    "Identify all images on the page and get their sizes.\n",
    "\n",
    "- **Comment**: Be aware that images may not always be stored in the same format. Ensure you construct the correct URLs for them.\n",
    "- **Hint**: You may need to check the attributes of the `<img>` tags to get additional information, such as the size of the images if available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b6cc74-d500-49d4-8e30-f7991d4e4fc6",
   "metadata": {},
   "source": [
    "##### Step 6: Extract Numbers, Dates, and Geographical Coordinates\n",
    "Identify numbers, dates, and geographical coordinates from the text.\n",
    "\n",
    "- **Comment**: Different formats for dates and numbers can complicate extraction. Consider the various ways these can appear on the page.\n",
    "- **Hint**: Use regular expressions tailored for specific patterns (e.g., date formats or geographic coordinates) to accurately identify them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4af9aa-0e9f-4af0-8eca-7c8378d58779",
   "metadata": {},
   "source": [
    "##### Step 7: Identify Proper Nouns\n",
    "Extract proper nouns from the text.\n",
    "\n",
    "- **Comment**: Proper nouns can include names of people, places, and organizations. Identifying them correctly can enhance your data analysis.\n",
    "- **Hint**: Use Natural Language Processing (NLP) techniques, such as named entity recognition, to automate the identification of proper nouns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db0cd42-246a-4d82-9961-2ea425022141",
   "metadata": {},
   "source": [
    "##### Step 8: Extract Structured Data (Tables)\n",
    "Identify and extract data from tables present in the HTML.\n",
    "\n",
    "- **Comment**: Tables often contain organized data that can be useful for analysis. Make sure to capture both header and data cells.\n",
    "- **Hint**: Familiarize yourself with the structure of HTML tables, including how to navigate rows (`<tr>`) and cells (`<td>` and `<th>`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f435c3-a4c5-4855-9ed7-64ea1d677079",
   "metadata": {},
   "source": [
    "##### Step 9: Differentiate Sections and Paragraphs\n",
    "Identify and separate sections and paragraphs in the content.\n",
    "\n",
    "- **Comment**: Sections help in understanding the organization of the content. Recognizing different heading levels can aid in content navigation.\n",
    "- **Hint**: Use appropriate tags (`<h1>`, `<h2>`, etc.) to differentiate between sections and ensure you capture their associated content, like paragraphs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249b58b4",
   "metadata": {},
   "source": [
    "## Exercise 4.2 \\[★★\\]\n",
    "\n",
    "Using the extracted text, build a cleaned corpus and compare word frequencies.\n",
    "\n",
    "1.  Remove punctuation, digits, and stopwords (French). You may use nltk.corpus.stopwords or spacy.lang.fr.stop_words.\n",
    "2.  Compute the top 20 most frequent words before and after stopword removal.\n",
    "3.  Plot a bar chart of the top 20 words after cleaning.\n",
    "\n",
    "Hint: Keep your preprocessing steps in a small function so you can reuse it.\n",
    "\n",
    "    from collections import Counter\n",
    "    import re\n",
    "\n",
    "    def tokenize(text):\n",
    "        # Your cleaning + tokenization here\n",
    "        return tokens\n",
    "\n",
    "    raw_tokens = tokenize(text)\n",
    "    clean_tokens = [t for t in raw_tokens if t not in stopwords]\n",
    "\n",
    "    print(Counter(raw_tokens).most_common(20))\n",
    "    print(Counter(clean_tokens).most_common(20))\n",
    "\n",
    "    # Plot the top 20 cleaned words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf88902-eb38-4e12-947e-1a7a60979948",
   "metadata": {},
   "source": [
    "## Exercise 4.3 \\[★★★\\]\n",
    "Analyze the text from the downloaded Wikipedia page by applying stemming, n-gram extraction, PoS tagging, lemmatization, morphological analysis, named entity recognition, and word embedding using Word2Vec models. Compare the results from NLTK, spaCy, and Gensim to evaluate their effectiveness in text analysis tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0da2ca0-e423-44d3-97f8-8141c3da6094",
   "metadata": {},
   "source": [
    "#### Prerequisites\n",
    "Make sure you have the required libraries installed. You can install them using pip if you haven't already:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b195a028-5472-49b6-9960-4cb53b93b820",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk spacy gensim wordcloud seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ff3539-b477-455e-a197-a82cca04cbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python -m spacy download fr_core_news_sm  # For French language processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a1ba2f-287a-44f4-84ff-2de1282936fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094eb7f3-92dc-4fc7-ba93-83e420cab4cf",
   "metadata": {},
   "source": [
    "#### Step 1: Load the Wikipedia Page\n",
    "Start by loading the HTML file you saved earlier and extracting the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb11a6e3-22f5-4d61-bf50-9ee8046a8b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load the HTML file\n",
    "with open(\"paris.html\", \"r\", encoding='utf-8') as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "text = soup.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9be759-1767-488b-9a73-74b658182c20",
   "metadata": {},
   "source": [
    "#### Step 2: Apply Stemming Algorithms\n",
    "Use the Porter and Snowball stemmers from NLTK to stem the words from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41fab36-d89b-4892-9a8d-9ae75092d72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Tokenize and clean the text\n",
    "words = re.findall(r'\\w+', text.lower())\n",
    "\n",
    "# Initialize stemmers\n",
    "porter_stemmer = PorterStemmer()\n",
    "snowball_stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# Apply stemming\n",
    "porter_stems = [porter_stemmer.stem(word) for word in words]\n",
    "snowball_stems = [snowball_stemmer.stem(word) for word in words]\n",
    "\n",
    "# Count unique stems\n",
    "porter_stem_count = Counter(porter_stems)\n",
    "snowball_stem_count = Counter(snowball_stems)\n",
    "\n",
    "# Display the most common stems and count of unique stems\n",
    "print(\"Most common Porter stems:\", porter_stem_count.most_common(10))\n",
    "print(\"Unique Porter stems count:\", len(porter_stem_count))\n",
    "\n",
    "print(\"Most common Snowball stems:\", snowball_stem_count.most_common(10))\n",
    "print(\"Unique Snowball stems count:\", len(snowball_stem_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edb693d-03b9-45b5-9f20-ce0d251bca00",
   "metadata": {},
   "source": [
    "#### Step 3: Extract N-grams\n",
    "Generate and display the most common n-grams (1-grams to 5-grams) from the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c7ce8f-84be-408e-aa62-8e3f0f4548a8",
   "metadata": {},
   "source": [
    "#### Step 4: Part-of-Speech (PoS) Tagging\n",
    "Use NLTK or spaCy to perform PoS tagging on the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083c6f0f-0cd2-411f-bf28-c62f94c6b144",
   "metadata": {},
   "source": [
    "#### Step 5: Lemmatization\n",
    "Apply lemmatization using NLTK or spaCy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19c3ad7-1c56-46f0-a23f-14457c7bb3ba",
   "metadata": {},
   "source": [
    "#### Step 6: Morphological Analysis\n",
    "Use spaCy to perform morphological analysis on the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069c24ae-c334-48e4-8831-5e14451677ab",
   "metadata": {},
   "source": [
    "#### Step 7: Named Entity Recognition (NER)\n",
    "Use spaCy to identify named entities in the text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3b8c1d-4cd8-490d-9560-15b8d2fe949f",
   "metadata": {},
   "source": [
    "#### Step 8: Frequency Distribution of Words\n",
    "Visualize the frequencydistribution of words using Matplotlib."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f386cadd-d6cb-4a86-8509-21f14fb9c813",
   "metadata": {},
   "source": [
    "#### Step 9: Create a Word Cloud\n",
    "\n",
    "Generate a word cloud to visualize the most frequent words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304f7f68-00b8-43d6-963e-d7b754859341",
   "metadata": {},
   "source": [
    "#### Step 10: Visualization of Named Entities\n",
    "\n",
    "Visualize the named entities recognized in the text using Matplotlib."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4f5a7b-84fe-4b13-ab15-cb0979fc5860",
   "metadata": {},
   "source": [
    "#### Step 11: Visualization of Most Common Nouns\n",
    "\n",
    "Visualize the most common nouns in the text, which can provide insights into the main subjects discussed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
