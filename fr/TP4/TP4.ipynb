{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e542386b-0558-4e40-b52c-de17d2181f85",
   "metadata": {},
   "source": [
    "# Travaux pratiques 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673e9851-e863-493d-9526-6f6aaf4aa8af",
   "metadata": {},
   "source": [
    "## Objectifs\n",
    "\n",
    "1.  Comprendre les techniques d'analyse de texte, y compris le stemming, la lemmatisation et l'analyse morphologique.\n",
    "2.  Pratiquer l'extraction d'éléments structurés à partir du HTML (liens, images, tableaux, sections).\n",
    "3.  Appliquer des outils NLP pour explorer un corpus de texte réel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582e0b36-39be-4ec6-ade3-387b77f5f5b4",
   "metadata": {},
   "source": [
    "## Exercice 4.1 \\[★\\]\n",
    "\n",
    "Téléchargez cette page Web de Wikipedia : [https://fr.wikipedia.org/wiki/Paris](https://fr.wikipedia.org/wiki/Paris) et enregistrez le fichier au format HTML. Analysez la page Wikipedia en extrayant et en comptant les mots, les liens, les images, les nombres, les dates, les noms propres et les données structurées à partir des tableaux, tout en différenciant les sections et les paragraphes. Cela implique de télécharger le HTML, de le parser et d'identifier systématiquement le contenu pertinent. Écrivez un programme pour mettre en œuvre ces tâches :\n",
    "\n",
    "1. **Télécharger le HTML** : Récupérer et enregistrer la page Wikipedia au format HTML.\n",
    "2. **Charger le Contenu** : Lire et parser le fichier HTML pour analyse.\n",
    "3. **Analyse des Mots** : Compter les occurrences de mots dans le texte.\n",
    "4. **Extraction des Liens** : Identifier et catégoriser les liens internes et externes.\n",
    "5. **Extraction d'Images** : Localiser les images et rassembler leurs URL et tailles.\n",
    "6. **Extraction de Nombres et de Dates** : Identifier les nombres, les dates et les coordonnées géographiques.\n",
    "7. **Noms Propres** : Extraire les noms de personnes et de lieux.\n",
    "8. **Données des Tableaux** : Localiser et extraire les données des tableaux.\n",
    "9. **Différenciation des Sections** : Identifier les sections et les paragraphes dans le contenu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f46833e-4bce-4dc9-8add-89f44a5c7492",
   "metadata": {},
   "source": [
    "#### Analyse de la Page Wikipedia : Paris\n",
    "\n",
    "Dans ce notebook, des tâches seront effectuées pour extraire et analyser divers éléments de la page Wikipedia de Paris.\n",
    "\n",
    "##### Étape 1 : Télécharger la Page HTML\n",
    "Tout d'abord, téléchargez le contenu HTML de la page Wikipedia spécifiée et enregistrez-le en tant que fichier HTML. Nous utilisons la bibliothèque `requests` pour gérer la requête HTTP. N'oubliez pas de vérifier le statut de la réponse pour confirmer que la page a été téléchargée avec succès."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0ab6df-7131-4d27-87de-ea431191a320",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# URL of the Wikipedia page\n",
    "url = \"https://fr.wikipedia.org/wiki/Paris\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Save the content as an HTML file\n",
    "with open(\"paris.html\", \"w\", encoding='utf-8') as file:\n",
    "    file.write(response.text)\n",
    "\n",
    "print(\"HTML page downloaded and saved as paris.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ebbfa5-7cff-41ab-818d-b94338ca42c5",
   "metadata": {},
   "source": [
    "##### Étape 2 : Charger le Contenu HTML\n",
    "Chargez le fichier HTML téléchargé pour une analyse plus approfondie.\n",
    "- **Commentaire** : Parser le HTML est crucial pour extraire des données. Assurez-vous d'utiliser une bibliothèque comme BeautifulSoup qui peut naviguer efficacement dans la structure HTML.\n",
    "\n",
    "Familiarisez-vous avec les méthodes de `BeautifulSoup` pour trouver des éléments dans le HTML, telles que `find()` et `find_all()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef264ae1-81a0-4975-ae4c-b689d8103dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load the HTML file\n",
    "with open(\"paris.html\", \"r\", encoding='utf-8') as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "print(\"HTML content loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fe0893-327f-4da6-bd18-d17288d044e6",
   "metadata": {},
   "source": [
    "##### Étape 3 : Extraire et Analyser les Mots\n",
    "Comptez les occurrences de chaque mot sur la page.\n",
    "- **Commentaire** : Envisagez de normaliser le texte en le convertissant en minuscules pour éviter de compter le même mot en différentes majuscules séparément. Nous utilisons des expressions régulières pour filtrer efficacement les caractères non alphanumériques lors de la séparation du texte en mots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b552f2d2-11ab-4f38-80b5-dbd900f20e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Extract text from the HTML content\n",
    "text = soup.get_text()\n",
    "\n",
    "# Clean and split text into words\n",
    "words = re.findall(r'\\w+', text.lower())\n",
    "word_count = Counter(words)\n",
    "\n",
    "# Display the 10 most common words\n",
    "print(word_count.most_common(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6c8782-5dfb-4bb2-a9f2-a10e365daf7b",
   "metadata": {},
   "source": [
    "##### Étape 4 : Extraire les Liens\n",
    "Identifiez tous les liens internes et externes de la page.\n",
    "\n",
    "- **Commentaire** : Comprendre la différence entre les liens internes et externes est important pour la catégorisation.\n",
    "- **Indice** : Vérifiez l'attribut `href` des balises d'ancrage (`<a>`) pour déterminer le type de lien."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7e9cae-5524-48cf-9801-25a3a5c89056",
   "metadata": {},
   "source": [
    "##### Étape 5 : Extraire les Images et Leurs Tailles\n",
    "Identifiez toutes les images sur la page et obtenez leurs tailles.\n",
    "\n",
    "- **Commentaire** : Soyez conscient que les images ne sont pas toujours stockées dans le même format. Assurez-vous de construire les bonnes URLs pour elles.\n",
    "- **Indice** : Vous devrez peut-être vérifier les attributs des balises `<img>` pour obtenir des informations supplémentaires, telles que la taille des images si disponible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b6cc74-d500-49d4-8e30-f7991d4e4fc6",
   "metadata": {},
   "source": [
    "##### Étape 6 : Extraire les Nombres, Dates et Coordonnées Géographiques\n",
    "Identifiez les nombres, dates et coordonnées géographiques dans le texte.\n",
    "\n",
    "- **Commentaire** : Différents formats pour les dates et les nombres peuvent compliquer l'extraction. Considérez les diverses manières dont ces éléments peuvent apparaître sur la page.\n",
    "- **Indice** : Utilisez des expressions régulières adaptées à des motifs spécifiques (par exemple, formats de date ou coordonnées géographiques) pour les identifier avec précision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4af9aa-0e9f-4af0-8eca-7c8378d58779",
   "metadata": {},
   "source": [
    "##### Étape 7 : Identifier les Noms Propres\n",
    "Extraire les noms propres du texte.\n",
    "\n",
    "- **Commentaire** : Les noms propres peuvent inclure des noms de personnes, de lieux et d'organisations. Les identifier correctement peut améliorer votre analyse des données.\n",
    "- **Indice** : Utilisez des techniques de traitement du langage naturel (NLP), telles que la reconnaissance d'entités nommées, pour automatiser l'identification des noms propres."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db0cd42-246a-4d82-9961-2ea425022141",
   "metadata": {},
   "source": [
    "##### Étape 8 : Extraire des Données Structurées (Tableaux)\n",
    "Identifiez et extrayez des données des tableaux présents dans le HTML.\n",
    "\n",
    "- **Commentaire** : Les tableaux contiennent souvent des données organisées qui peuvent être utiles pour l'analyse. Assurez-vous de capturer à la fois les cellules d'en-tête et les cellules de données.\n",
    "- **Indice** : Familiarisez-vous avec la structure des tableaux HTML, y compris comment naviguer dans les lignes (`<tr>`) et les cellules (`<td>` et `<th>`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f435c3-a4c5-4855-9ed7-64ea1d677079",
   "metadata": {},
   "source": [
    "##### Étape 9 : Différencier les Sections et les Paragraphes\n",
    "Identifiez et séparez les sections et les paragraphes dans le contenu.\n",
    "\n",
    "- **Commentaire** : Les sections aident à comprendre l'organisation du contenu. Reconnaître les différents niveaux de titres peut faciliter la navigation dans le contenu.\n",
    "- **Indice** : Utilisez les balises appropriées (`<h1>`, `<h2>`, etc.) pour différencier les sections et assurez-vous de capturer leur contenu associé, comme les paragraphes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 4.2 \\[★★\\]\n",
    "\n",
    "En utilisant le texte extrait, construisez un corpus nettoyé et comparez les fréquences de mots.\n",
    "\n",
    "1.  Supprimez la ponctuation, les chiffres et les mots vides (français). Vous pouvez utiliser nltk.corpus.stopwords ou spacy.lang.fr.stop_words.\n",
    "2.  Calculez les 20 mots les plus fréquents avant et après suppression des mots vides.\n",
    "3.  Tracez un graphique en barres des 20 mots les plus fréquents après nettoyage.\n",
    "\n",
    "Indice : Gardez vos étapes de prétraitement dans une petite fonction afin de pouvoir la réutiliser.\n",
    "\n",
    "    from collections import Counter\n",
    "    import re\n",
    "\n",
    "    def tokenize(text):\n",
    "        # Votre nettoyage + tokenisation ici\n",
    "        return tokens\n",
    "\n",
    "    raw_tokens = tokenize(text)\n",
    "    clean_tokens = [t for t in raw_tokens if t not in stopwords]\n",
    "\n",
    "    print(Counter(raw_tokens).most_common(20))\n",
    "    print(Counter(clean_tokens).most_common(20))\n",
    "\n",
    "    # Tracer les 20 mots nettoyés\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf88902-eb38-4e12-947e-1a7a60979948",
   "metadata": {},
   "source": [
    "## Exercice 4.3 \\[★★★\\]\n",
    "Analysez le texte de la page Wikipedia téléchargée en appliquant le stemming, l'extraction d'n-grammes, l'étiquetage des parties du discours (PoS), la lemmatisation, l'analyse morphologique et la reconnaissance d'entités nommées. Comparez les résultats de NLTK et de spaCy pour évaluer leur efficacité dans les tâches d'analyse de texte."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0da2ca0-e423-44d3-97f8-8141c3da6094",
   "metadata": {},
   "source": [
    "### Prérequis\n",
    "\n",
    "Assurez-vous d'avoir les bibliothèques nécessaires installées. Vous pouvez les installer en utilisant pip si ce n'est pas déjà fait :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b195a028-5472-49b6-9960-4cb53b93b820",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk spacy gensim wordcloud seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ff3539-b477-455e-a197-a82cca04cbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python -m spacy download fr_core_news_sm  # For French language processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473610f4-cd26-4843-8efb-2777ce875391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094eb7f3-92dc-4fc7-ba93-83e420cab4cf",
   "metadata": {},
   "source": [
    "#### Étape 1 : Charger la Page Wikipedia\n",
    "Commencez par charger le fichier HTML que vous avez enregistré précédemment et extraire le texte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb11a6e3-22f5-4d61-bf50-9ee8046a8b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load the HTML file\n",
    "with open(\"paris.html\", \"r\", encoding='utf-8') as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "text = soup.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9be759-1767-488b-9a73-74b658182c20",
   "metadata": {},
   "source": [
    "#### Étape 2 : Appliquer des Algorithmes de Stemming\n",
    "Utilisez les stemmers Porter et Snowball de NLTK pour réduire les mots du texte à leur racine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41fab36-d89b-4892-9a8d-9ae75092d72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Tokenize and clean the text\n",
    "words = re.findall(r'\\w+', text.lower())\n",
    "\n",
    "# Initialize stemmers\n",
    "porter_stemmer = PorterStemmer()\n",
    "snowball_stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# Apply stemming\n",
    "porter_stems = [porter_stemmer.stem(word) for word in words]\n",
    "snowball_stems = [snowball_stemmer.stem(word) for word in words]\n",
    "\n",
    "# Count unique stems\n",
    "porter_stem_count = Counter(porter_stems)\n",
    "snowball_stem_count = Counter(snowball_stems)\n",
    "\n",
    "# Display the most common stems and count of unique stems\n",
    "print(\"Most common Porter stems:\", porter_stem_count.most_common(10))\n",
    "print(\"Unique Porter stems count:\", len(porter_stem_count))\n",
    "\n",
    "print(\"Most common Snowball stems:\", snowball_stem_count.most_common(10))\n",
    "print(\"Unique Snowball stems count:\", len(snowball_stem_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edb693d-03b9-45b5-9f20-ce0d251bca00",
   "metadata": {},
   "source": [
    "#### Étape 3 : Extraire des N-grammes\n",
    "Générez et affichez les n-grammes les plus courants (1-grammes à 5-grammes) du texte."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c7ce8f-84be-408e-aa62-8e3f0f4548a8",
   "metadata": {},
   "source": [
    "#### Étape 4 : Étiquetage des Parties du Discours (PoS)\n",
    "Utilisez NLTK ou spaCy pour effectuer l'étiquetage des parties du discours sur le texte."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083c6f0f-0cd2-411f-bf28-c62f94c6b144",
   "metadata": {},
   "source": [
    "#### Étape 5 : Lemmatisation\n",
    "Appliquez la lemmatisation en utilisant NLTK ou spaCy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19c3ad7-1c56-46f0-a23f-14457c7bb3ba",
   "metadata": {},
   "source": [
    "#### Étape 6 : Analyse Morphologique\n",
    "Utilisez spaCy pour effectuer une analyse morphologique sur le texte."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069c24ae-c334-48e4-8831-5e14451677ab",
   "metadata": {},
   "source": [
    "#### Étape 7 : Reconnaissance d'Entités Nommées (NER)\n",
    "Utilisez spaCy pour identifier les entités nommées dans le texte."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3b8c1d-4cd8-490d-9560-15b8d2fe949f",
   "metadata": {},
   "source": [
    "#### Étape 8 : Distribution de Fréquence des Mots\n",
    "Visualisez la distribution de fréquence des mots à l'aide de Matplotlib."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f386cadd-d6cb-4a86-8509-21f14fb9c813",
   "metadata": {},
   "source": [
    "#### Étape 9 : Créer un Nuage de Mots\n",
    "\n",
    "Générez un nuage de mots pour visualiser les mots les plus fréquents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304f7f68-00b8-43d6-963e-d7b754859341",
   "metadata": {},
   "source": [
    "#### Étape 10 : Visualisation des Entités Nommées\n",
    "\n",
    "Visualisez les entités nommées reconnues dans le texte à l'aide de Matplotlib."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4f5a7b-84fe-4b13-ab15-cb0979fc5860",
   "metadata": {},
   "source": [
    "#### Étape 11 : Visualisation des Noms les Plus Courants\n",
    "\n",
    "Visualisez les noms les plus courants dans le texte, ce qui peut fournir des informations sur les principaux sujets abordés."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
